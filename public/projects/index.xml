<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Systems for AI Lab</title>
    <link>/projects/</link>
      <atom:link href="/projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Georgia Tech Systems for Artificial Intelligence Lab</copyright><lastBuildDate>Sat, 24 Apr 2021 15:55:22 -0400</lastBuildDate>
    <image>
      <url>/images/logo_hu593886a39899d4c673c949ed243a190d_66707_300x300_fit_lanczos_2.png</url>
      <title>Projects</title>
      <link>/projects/</link>
    </image>
    
    <item>
      <title>CompOFA: Compound Once-For-All Networks for Faster Multi-Platform Deployment</title>
      <link>/projects/compofa/</link>
      <pubDate>Sat, 24 Apr 2021 15:55:22 -0400</pubDate>
      <guid>/projects/compofa/</guid>
      <description>

















&lt;figure id=&#34;figure-conventional-training-current-sota-and-compofa&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/overview.png&#34; data-caption=&#34;Conventional training, current SOTA, and CompOFA&#34;&gt;


  &lt;img src=&#34;./img/overview.png&#34; alt=&#34;&#34; width=&#34;100%&#34; &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Conventional training, current SOTA, and CompOFA
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The emergence of CNNs in mainstream deployment has necessitated methods to design and train efficient architectures tailored to maximize the accuracy under diverse hardware &amp;amp; latency constraints.&lt;/p&gt;
&lt;p&gt;Designing and training DNN architectures for each deployment target is not feasible. Each deployment costs training time, compute dollars, system expertise, ML expertise, CO2 emissions.&lt;/p&gt;
&lt;p&gt;In &lt;em&gt;CompOFA&lt;/em&gt;, we propose a cost-effective and faster technique to build model families that support multiple deployment platforms. Using insights from model design and system deployment, we build upon the current best methods that take 40-50 GPU days of computation and make their training and searching processes &lt;strong&gt;faster by 2x and 200x&lt;/strong&gt;, respectively – all while building a family of equally efficient and diverse models!&lt;/p&gt;
&lt;h3 id=&#34;compofa-matches-efficiency-and-diversity-of-sota-methods&#34;&gt;CompOFA matches efficiency and diversity of SOTA methods&amp;hellip;&lt;/h3&gt;


















&lt;figure id=&#34;figure-efficient-model-families-for-diverse-hardwares----from-mobile-phones-to-gpus&#34;&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/pareto-results.png&#34; data-caption=&#34;Efficient model families for diverse hardwares &amp;amp;ndash; from mobile phones to GPUs&#34;&gt;


  &lt;img src=&#34;./img/pareto-results.png&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Efficient model families for diverse hardwares &amp;ndash; from mobile phones to GPUs
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;with-2x-faster-training-and-216x-faster-searching&#34;&gt;&amp;hellip;with &lt;em&gt;2x&lt;/em&gt; faster training and &lt;em&gt;216x&lt;/em&gt; faster searching&lt;/h3&gt;


















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/table.png&#34; &gt;


  &lt;img src=&#34;./img/table.png&#34; alt=&#34;&#34; width=&#34;40%&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;better-overall-average-accuracy&#34;&gt;Better overall average accuracy&lt;/h3&gt;
&lt;p&gt;At a population level, CompOFA has a higher concentration of accurate models&lt;/p&gt;


















&lt;figure &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/avg_accuracy.png&#34; &gt;


  &lt;img src=&#34;./img/avg_accuracy.png&#34; alt=&#34;&#34; width=&#34;70%&#34; &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;h1 id=&#34;learn-more&#34;&gt;Learn more&lt;/h1&gt;
&lt;p&gt;Please check out our &lt;a href=&#34;https://arxiv.org/abs/2104.12642&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;paper&lt;/a&gt; and &lt;a href=&#34;https://iclr.cc/media/PosterPDFs/ICLR%202021/2c3ddf4bf13852db711dd1901fb517fa.png&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;poster&lt;/a&gt; at ICLR 2021! Our code and pretrained models are also available on our &lt;a href=&#34;https://github.com/gatech-sysml/compofa&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github repository&lt;/a&gt;. Also check out our &lt;a href=&#34;/compofa/blog&#34;&gt;blog post&lt;/a&gt;!&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-bibtex&#34;&gt;@inproceedings{compofa-iclr21,
  author    = {Manas Sahni and Shreya Varshini and Alind Khare and
               Alexey Tumanov},
  title     = {{C}omp{OFA}: Compound Once-For-All Networks for Faster Multi-Platform Deployment},
  month     = {May},
  booktitle = {Proc. of the 9th International Conference on Learning Representations},
  series = {ICLR &#39;21},
  year = {2021},
  url       = {https://openreview.net/forum?id=IgIk8RRT-Z}
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
